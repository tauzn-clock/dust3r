{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dust3r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#Set current location to the location of the script\n",
    "os.chdir(\"/dust3r\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Warning, cannot find cuda-compiled version of RoPE2D, using a slow pytorch version instead\n"
     ]
    }
   ],
   "source": [
    "#Display imgs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "import torch\n",
    "\n",
    "from dust3r.inference import inference, inference_with_mask\n",
    "from dust3r.model import AsymmetricCroCo3DStereo\n",
    "from dust3r.utils.image import load_images\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n",
    "\n",
    "DATA_PATH = \"/dust3r/masked_dust3r/data/jackal_training_data_0\"\n",
    "IMG_FILE_EXTENSION = \".png\"\n",
    "MASK_FILE_EXTENSION = \".png\"\n",
    "device = 'cuda'\n",
    "batch_size = 1\n",
    "schedule = 'cosine'\n",
    "lr = 0.01\n",
    "niter = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading model from checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\n",
      "instantiating : AsymmetricCroCo3DStereo(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100', patch_embed_cls='PatchEmbedDust3R', img_size=(512, 512), head_type='dpt', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), landscape_only=False)\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model_name = \"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "# you can put the path to a local checkpoint in model_name if needed\n",
    "model = AsymmetricCroCo3DStereo.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading a list of 15 images\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/0.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/1.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/2.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/3.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/4.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/5.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/6.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/7.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/8.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/9.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/10.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/11.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/12.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/13.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/14.png with resolution 1280x720 --> 512x288\n",
      " (Found 15 images)\n"
     ]
    }
   ],
   "source": [
    "# load_images can take a list of images or a directory\n",
    "\n",
    "images_array = []\n",
    "masks_array = []\n",
    "\n",
    "for i in range(15):\n",
    "    images_array.append(os.path.join(DATA_PATH,\"masked_images/{}{}\".format(i,IMG_FILE_EXTENSION)))\n",
    "    masks_array.append(os.path.join(DATA_PATH,\"masks/{}{}\".format(i,MASK_FILE_EXTENSION)))\n",
    "images = load_images(images_array, size=512, verbose=True)\n",
    "\n",
    "masks = []\n",
    "\n",
    "for i in range(len(masks_array)):\n",
    "    #Open as mask and load to gpu\n",
    "    mask = Image.open(masks_array[i]).convert('L')\n",
    "    #Resize to match image size\n",
    "    _,_,H,W = images[i][\"img\"].shape\n",
    "    mask = mask.resize((W,H))\n",
    "\n",
    "    mask = np.array(mask)\n",
    "    mask = torch.tensor(mask).to(device)/255\n",
    "    masks.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Inference with model on 210 image pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/210 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 210/210 [01:26<00:00,  2.44it/s]\n"
     ]
    }
   ],
   "source": [
    "pairs = make_pairs(images, scene_graph='complete', prefilter=None, symmetrize=True)\n",
    "output = inference_with_mask(pairs, model, device, masks, batch_size=batch_size)\n",
    "#output = inference(pairs, model, device, batch_size=batch_size)\n",
    "\n",
    "view1, pred1 = output['view1'], output['pred1']\n",
    "view2, pred2 = output['view2'], output['pred2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " init edge (3*,5*) score=1.2058569192886353\n",
      " init edge (3,6*) score=1.2041503190994263\n",
      " init edge (3,8*) score=1.192130208015442\n",
      " init edge (4*,8) score=1.1913957595825195\n",
      " init edge (3,7*) score=1.1802425384521484\n",
      " init edge (3,9*) score=1.168657660484314\n",
      " init edge (3,2*) score=1.1556755304336548\n",
      " init edge (4,10*) score=1.154893398284912\n",
      " init edge (1*,6) score=1.1510189771652222\n",
      " init edge (5,11*) score=1.1341043710708618\n",
      " init edge (6,0*) score=1.1211603879928589\n",
      " init edge (6,14*) score=1.096208095550537\n",
      " init edge (6,12*) score=1.0902040004730225\n",
      " init edge (6,13*) score=1.0545775890350342\n",
      " init loss = 0.00020294601563364267\n",
      "Global alignement - optimizing for:\n",
      "['pw_poses', 'im_depthmaps', 'im_poses', 'im_focals']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:35<00:00,  3.13it/s, lr=1.27413e-06 loss=7.841e-05]  \n"
     ]
    }
   ],
   "source": [
    "scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PointCloudOptimizer)\n",
    "loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)\n",
    "\n",
    "imgs = scene.imgs\n",
    "focals = scene.get_focals()\n",
    "poses = scene.get_im_poses()\n",
    "pts3d = scene.get_pts3d()\n",
    "confidence_masks = scene.get_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] Write PLY failed: point cloud has 0 points.\u001b[0;m\n"
     ]
    }
   ],
   "source": [
    "#Check if pointclouds folder exists\n",
    "#If exists, delete all files in the folder\n",
    "if os.path.exists(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH)):\n",
    "    for file in os.listdir(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH)):\n",
    "        os.remove(\"{DATA_PATH}/pointclouds/{file}\".format(DATA_PATH=DATA_PATH, file=file))\n",
    "        \n",
    "if not os.path.exists(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH)):\n",
    "    os.makedirs(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH))\n",
    "\n",
    "for i in range(len(images)):\n",
    "    pointcloud = pts3d[i].detach().cpu().numpy()\n",
    "    pointcloud = pointcloud.reshape(-1, 3)\n",
    "    color = imgs[i].reshape(-1, 3)\n",
    "    confidence_mask = confidence_masks[i].detach().cpu().numpy()\n",
    "    confidence_mask = confidence_mask.reshape(-1)\n",
    "    \n",
    "    masked_pointcloud = []\n",
    "    masked_color = []\n",
    "\n",
    "    for j in range(len(confidence_mask)):\n",
    "        if confidence_mask[j]:\n",
    "            masked_pointcloud.append(pointcloud[j])\n",
    "            masked_color.append(color[j])\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(masked_pointcloud)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(masked_color)\n",
    "    o3d.io.write_point_cloud(\"{DATA_PATH}/pointclouds/pointcloud{i}.ply\".format(DATA_PATH=DATA_PATH, i=i), pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 511.5, 287.5, -0.5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf1 = confidence_masks[2].cpu().numpy()\n",
    "conf2 = confidence_masks[3].cpu().numpy()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(conf1, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(conf2, cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#Create transform file\n",
    "#TODO: Per frame camera model?\n",
    "transform = {}\n",
    "transform[\"camera_model\"] = \"OPENCV\"\n",
    "\n",
    "averge_focal = focals.sum()/len(focals)\n",
    "transform[\"fl_x\"] = averge_focal.item()\n",
    "transform[\"fl_y\"] = averge_focal.item()\n",
    "\n",
    "#Find size of images\n",
    "img = Image.open(images_array[0])\n",
    "width, height = img.size\n",
    "transform[\"w\"] = width\n",
    "transform[\"h\"] = height\n",
    "transform[\"c_x\"] = width//2\n",
    "transform[\"c_y\"] = height//2\n",
    "\n",
    "transform[\"frames\"] = []\n",
    "\n",
    "for i in range(len(poses)):\n",
    "    frame = {}\n",
    "    frame[\"file_path\"] = \"images/{}{}\".format(i,IMG_FILE_EXTENSION)\n",
    "    frame[\"transform_matrix\"] = poses[i].detach().cpu().numpy().tolist()\n",
    "    frame[\"mask_path\"] = \"masks/{}{}\".format(i,MASK_FILE_EXTENSION)\n",
    "    transform[\"frames\"].append(frame)\n",
    "\n",
    "#Save transform file\n",
    "with open(\"{}/transforms.json\".format(DATA_PATH), 'w') as f:\n",
    "    json.dump(transform, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
