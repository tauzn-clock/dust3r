{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dust3r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/dust3r\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import open3d as o3d\n",
    "\n",
    "from dust3r.inference import inference_with_mask, create_gaussian_kernel\n",
    "from dust3r.model import AsymmetricCroCo3DStereo\n",
    "from dust3r.utils.image import load_images\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n",
    "from dust3r.cloud_opt.base_opt import global_alignment_loop\n",
    "from masked_dust3r.scripts.utils.math import *\n",
    "from masked_dust3r.scripts.utils.image import *\n",
    "\n",
    "\n",
    "DATA_PATH = \"/dust3r/masked_dust3r/data/jackal_training_data_0\"\n",
    "IMG_FILE_EXTENSION = \".png\"\n",
    "MASK_FILE_EXTENSION = \".png\"\n",
    "\n",
    "INIT_FRAMES = 50\n",
    "NEW_FRAMES = 10\n",
    "PREVIOUS_FRAMES = 40\n",
    "TOTAL_FRAMES = 300\n",
    "\n",
    "INIT_WEIGHT_FOCAL = 0.01 * 0\n",
    "INIT_WEIGHT_Z = 0.1 * 0\n",
    "INIT_WEIGHT_ROT = 0.1 * 0\n",
    "INIT_WEIGHT_TRANS_SMOOTHNESS = 0.001 * 0\n",
    "INIT_WEIGHT_ROT_SMOOTHNESS = 0.001 * 0\n",
    "\n",
    "NEW_WEIGHT_FOCAL = 0.1 * 0\n",
    "NEW_WEIGHT_Z = 0.1\n",
    "NEW_WEIGHT_ROT = 0.1\n",
    "NEW_WEIGHT_TRANS_SMOOTHNESS = 0.0001\n",
    "NEW_WEIGHT_ROT_SMOOTHNESS = 0.00001\n",
    "\n",
    "USE_COMMON_INTRINSICS = False\n",
    "\n",
    "device = 'cuda'\n",
    "batch_size = 1\n",
    "schedule = 'cosine'\n",
    "lr = 0.01\n",
    "niter = 300 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAUSSIAN_SIGMA = 21.0\n",
    "SIZE = int(GAUSSIAN_SIGMA * 3)\n",
    "\n",
    "kernel = create_gaussian_kernel(SIZE, GAUSSIAN_SIGMA).to(device)\n",
    "\n",
    "SIZE = 1\n",
    "kernel = torch.ones(SIZE, SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading model from checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiating : AsymmetricCroCo3DStereo(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100', patch_embed_cls='PatchEmbedDust3R', img_size=(512, 512), head_type='dpt', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), landscape_only=False)\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "\n",
    "model_name = \"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "# you can put the path to a local checkpoint in model_name if needed\n",
    "model = AsymmetricCroCo3DStereo.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading a list of 50 images\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/0.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/1.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/2.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/3.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/4.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/5.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/6.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/7.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/8.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/9.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/10.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/11.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/12.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/13.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/14.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/15.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/16.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/17.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/18.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/19.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/20.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/21.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/22.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/23.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/24.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/25.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/26.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/27.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/28.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/29.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/30.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/31.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/32.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/33.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/34.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/35.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/36.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/37.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/38.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/39.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/40.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/41.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/42.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/43.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/44.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/45.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/46.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/47.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/48.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/49.png with resolution 1280x720 --> 512x288\n",
      " (Found 50 images)\n"
     ]
    }
   ],
   "source": [
    "images_array = []\n",
    "masks_array = []\n",
    "\n",
    "for i in range(0,50):\n",
    "    images_array.append(os.path.join(DATA_PATH,\"masked_images/{}{}\".format(i,IMG_FILE_EXTENSION)))\n",
    "    masks_array.append(os.path.join(DATA_PATH,\"masks/{}{}\".format(i,MASK_FILE_EXTENSION)))\n",
    "images = load_images(images_array, size=512, verbose=True)\n",
    "_,_,H,W = images[0][\"img\"].shape\n",
    "masks = load_masks(masks_array, H, W, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Inference with model on 100 image pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:29<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "pairs = make_pairs(images, scene_graph='swin-1', prefilter=None, symmetrize=True)\n",
    "output = inference_with_mask(pairs, model, device, masks, kernel, batch_size=batch_size)\n",
    "del pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " init edge (31*,32*) score=1.5550881624221802\n",
      " init edge (32,33*) score=1.5348498821258545\n",
      " init edge (31,30*) score=1.511637568473816\n",
      " init edge (30,29*) score=1.5006829500198364\n",
      " init edge (33,34*) score=1.4510823488235474\n",
      " init edge (35*,34) score=1.4219846725463867\n",
      " init edge (36*,35) score=1.3642923831939697\n",
      " init edge (29,28*) score=1.3519669771194458\n",
      " init edge (27*,28) score=1.3290653228759766\n",
      " init edge (37*,36) score=1.229400873184204\n",
      " init edge (26*,27) score=1.3301632404327393\n",
      " init edge (26,25*) score=1.2620265483856201\n",
      " init edge (25,24*) score=1.244730830192566\n",
      " init edge (23*,24) score=1.2740055322647095\n",
      " init edge (22*,23) score=1.3368784189224243\n",
      " init edge (21*,22) score=1.3375743627548218\n",
      " init edge (20*,21) score=1.4055798053741455\n",
      " init edge (19*,20) score=1.5015896558761597\n",
      " init edge (19,18*) score=1.4620022773742676\n",
      " init edge (18,17*) score=1.4963304996490479\n",
      " init edge (17,16*) score=1.4511840343475342\n",
      " init edge (15*,16) score=1.4046778678894043\n",
      " init edge (14*,15) score=1.3500778675079346\n",
      " init edge (13*,14) score=1.294365644454956\n",
      " init edge (13,12*) score=1.2204217910766602\n",
      " init edge (12,11*) score=1.2504773139953613\n",
      " init edge (11,10*) score=1.310567021369934\n",
      " init edge (9*,10) score=1.383121371269226\n",
      " init edge (8*,9) score=1.4425055980682373\n",
      " init edge (8,7*) score=1.4684375524520874\n",
      " init edge (6*,7) score=1.5010648965835571\n",
      " init edge (5*,6) score=1.530758261680603\n",
      " init edge (4*,5) score=1.4789453744888306\n",
      " init edge (3*,4) score=1.4785699844360352\n",
      " init edge (2*,3) score=1.443853497505188\n",
      " init edge (1*,2) score=1.352523922920227\n",
      " init edge (0*,1) score=1.2875934839248657\n",
      " init edge (0,49*) score=1.2279117107391357\n",
      " init edge (48*,49) score=1.2657769918441772\n",
      " init edge (47*,48) score=1.3436837196350098\n",
      " init edge (46*,47) score=1.4093374013900757\n",
      " init edge (45*,46) score=1.4307368993759155\n",
      " init edge (44*,45) score=1.5154578685760498\n",
      " init edge (43*,44) score=1.5240027904510498\n",
      " init edge (42*,43) score=1.4834163188934326\n",
      " init edge (41*,42) score=1.4278193712234497\n",
      " init edge (41,40*) score=1.439340591430664\n",
      " init edge (39*,40) score=1.3084681034088135\n",
      " init edge (38*,39) score=1.2526692152023315\n",
      " init loss = 0.0001560822711326182\n",
      "Global alignement - optimizing for:\n",
      "['pw_poses', 'im_depthmaps.0', 'im_depthmaps.1', 'im_depthmaps.2', 'im_depthmaps.3', 'im_depthmaps.4', 'im_depthmaps.5', 'im_depthmaps.6', 'im_depthmaps.7', 'im_depthmaps.8', 'im_depthmaps.9', 'im_depthmaps.10', 'im_depthmaps.11', 'im_depthmaps.12', 'im_depthmaps.13', 'im_depthmaps.14', 'im_depthmaps.15', 'im_depthmaps.16', 'im_depthmaps.17', 'im_depthmaps.18', 'im_depthmaps.19', 'im_depthmaps.20', 'im_depthmaps.21', 'im_depthmaps.22', 'im_depthmaps.23', 'im_depthmaps.24', 'im_depthmaps.25', 'im_depthmaps.26', 'im_depthmaps.27', 'im_depthmaps.28', 'im_depthmaps.29', 'im_depthmaps.30', 'im_depthmaps.31', 'im_depthmaps.32', 'im_depthmaps.33', 'im_depthmaps.34', 'im_depthmaps.35', 'im_depthmaps.36', 'im_depthmaps.37', 'im_depthmaps.38', 'im_depthmaps.39', 'im_depthmaps.40', 'im_depthmaps.41', 'im_depthmaps.42', 'im_depthmaps.43', 'im_depthmaps.44', 'im_depthmaps.45', 'im_depthmaps.46', 'im_depthmaps.47', 'im_depthmaps.48', 'im_depthmaps.49', 'im_poses.0', 'im_poses.1', 'im_poses.2', 'im_poses.3', 'im_poses.4', 'im_poses.5', 'im_poses.6', 'im_poses.7', 'im_poses.8', 'im_poses.9', 'im_poses.10', 'im_poses.11', 'im_poses.12', 'im_poses.13', 'im_poses.14', 'im_poses.15', 'im_poses.16', 'im_poses.17', 'im_poses.18', 'im_poses.19', 'im_poses.20', 'im_poses.21', 'im_poses.22', 'im_poses.23', 'im_poses.24', 'im_poses.25', 'im_poses.26', 'im_poses.27', 'im_poses.28', 'im_poses.29', 'im_poses.30', 'im_poses.31', 'im_poses.32', 'im_poses.33', 'im_poses.34', 'im_poses.35', 'im_poses.36', 'im_poses.37', 'im_poses.38', 'im_poses.39', 'im_poses.40', 'im_poses.41', 'im_poses.42', 'im_poses.43', 'im_poses.44', 'im_poses.45', 'im_poses.46', 'im_poses.47', 'im_poses.48', 'im_poses.49', 'im_focals.0', 'im_focals.1', 'im_focals.2', 'im_focals.3', 'im_focals.4', 'im_focals.5', 'im_focals.6', 'im_focals.7', 'im_focals.8', 'im_focals.9', 'im_focals.10', 'im_focals.11', 'im_focals.12', 'im_focals.13', 'im_focals.14', 'im_focals.15', 'im_focals.16', 'im_focals.17', 'im_focals.18', 'im_focals.19', 'im_focals.20', 'im_focals.21', 'im_focals.22', 'im_focals.23', 'im_focals.24', 'im_focals.25', 'im_focals.26', 'im_focals.27', 'im_focals.28', 'im_focals.29', 'im_focals.30', 'im_focals.31', 'im_focals.32', 'im_focals.33', 'im_focals.34', 'im_focals.35', 'im_focals.36', 'im_focals.37', 'im_focals.38', 'im_focals.39', 'im_focals.40', 'im_focals.41', 'im_focals.42', 'im_focals.43', 'im_focals.44', 'im_focals.45', 'im_focals.46', 'im_focals.47', 'im_focals.48', 'im_focals.49']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:15<00:00,  2.21it/s, lr=1.27413e-06 loss=5.27541e-05]\n"
     ]
    }
   ],
   "source": [
    "#init_scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PlanePointCloudOptimizer)\n",
    "#loss = init_scene.compute_global_alignment(init=\"mst\", niter=niter, schedule='cosine', lr=lr)\n",
    "\n",
    "#scene = init_scene\n",
    "\n",
    "scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PlanePointCloudOptimizer, \n",
    "                        weight_focal = INIT_WEIGHT_FOCAL,\n",
    "                        weight_z = INIT_WEIGHT_Z ,\n",
    "                        weight_rot = INIT_WEIGHT_ROT  ,\n",
    "                        weight_trans_smoothness = INIT_WEIGHT_TRANS_SMOOTHNESS,\n",
    "                        weight_rot_smoothness = INIT_WEIGHT_ROT_SMOOTHNESS)\n",
    "#scene.im_poses = calculate_new_params(init_scene.im_poses,device)\n",
    "#scene.im_focals = init_scene.im_focals\n",
    "loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)\n",
    "\n",
    "#averge_focal = scene.get_focals().sum().item()/len(images_array)\n",
    "#fixed_focal = [averge_focal for _ in range(len(images_array))]\n",
    "#mask = [True for _ in range(len(images_array))]\n",
    "#scene.preset_focal(fixed_focal, mask)\n",
    "#loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)\n",
    "\n",
    "#scene.weight_focal = INIT_WEIGHT_FOCAL\n",
    "#scene.weight_z = INIT_WEIGHT_Z \n",
    "#scene.weight_rot = INIT_WEIGHT_ROT\n",
    "#scene.weight_trans_smoothness = INIT_WEIGHT_TRANS_SMOOTHNESS \n",
    "#scene.weight_rot_smoothness = INIT_WEIGHT_ROT_SMOOTHNESS\n",
    "#loss = global_alignment_loop(scene, niter=niter, schedule=schedule, lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = scene.imgs\n",
    "focals = scene.get_focals()\n",
    "poses = scene.get_im_poses()\n",
    "pts3d = scene.get_pts3d()\n",
    "confidence_masks = scene.get_masks()\n",
    "intrinsics = scene.get_intrinsics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[654.3383],\n",
      "        [574.3328],\n",
      "        [529.8439],\n",
      "        [535.4302],\n",
      "        [539.2348],\n",
      "        [526.7123],\n",
      "        [523.6014],\n",
      "        [528.8171],\n",
      "        [536.1041],\n",
      "        [538.9911],\n",
      "        [544.0812],\n",
      "        [629.2317],\n",
      "        [626.0013],\n",
      "        [628.2543],\n",
      "        [595.1767],\n",
      "        [543.0217],\n",
      "        [541.6720],\n",
      "        [534.4133],\n",
      "        [535.7574],\n",
      "        [526.4504],\n",
      "        [540.6248],\n",
      "        [603.9789],\n",
      "        [606.9436],\n",
      "        [656.5286],\n",
      "        [672.1915],\n",
      "        [668.3292],\n",
      "        [634.0631],\n",
      "        [621.3911],\n",
      "        [586.8094],\n",
      "        [555.3392],\n",
      "        [523.9168],\n",
      "        [521.4434],\n",
      "        [520.7944],\n",
      "        [532.0732],\n",
      "        [523.2665],\n",
      "        [533.1567],\n",
      "        [549.1788],\n",
      "        [639.7050],\n",
      "        [642.2066],\n",
      "        [629.8137],\n",
      "        [551.8750],\n",
      "        [533.1238],\n",
      "        [543.5737],\n",
      "        [526.3262],\n",
      "        [524.2938],\n",
      "        [533.7226],\n",
      "        [549.5974],\n",
      "        [609.3350],\n",
      "        [642.0872],\n",
      "        [625.8572]], device='cuda:0', grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(focals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if pointclouds folder exists\n",
    "#If exists, delete all files in the folder\n",
    "if os.path.exists(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH)):\n",
    "    for file in os.listdir(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH)):\n",
    "        os.remove(\"{DATA_PATH}/pointclouds/{file}\".format(DATA_PATH=DATA_PATH, file=file))\n",
    "        \n",
    "if not os.path.exists(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH)):\n",
    "    os.makedirs(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH))\n",
    "\n",
    "for i in range(len(images)):\n",
    "    pointcloud = pts3d[i].detach().cpu().numpy()\n",
    "    pointcloud = pointcloud.reshape(-1, 3)\n",
    "    color = imgs[i].reshape(-1, 3)\n",
    "    confidence_mask = confidence_masks[i].detach().cpu().numpy()\n",
    "    confidence_mask = confidence_mask.reshape(-1)\n",
    "    \n",
    "    masked_pointcloud = []\n",
    "    masked_color = []\n",
    "\n",
    "    for j in range(len(confidence_mask)):\n",
    "        if confidence_mask[j]:\n",
    "            masked_pointcloud.append(pointcloud[j])\n",
    "            masked_color.append(color[j])\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(masked_pointcloud)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(masked_color)\n",
    "\n",
    "    o3d.io.write_point_cloud(\"{DATA_PATH}/pointclouds/pointcloud{i}.ply\".format(DATA_PATH=DATA_PATH, i=int(re.split(\"\\.|/\",images_array[i])[-2])), pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create transform file\n",
    "\n",
    "img = Image.open(images_array[0])\n",
    "width, height = img.size\n",
    "RESCALE_FACTOR = 1 #width/512\n",
    "FIXED_FOCAL_LENGTH = 4.74\n",
    "\n",
    "transforms = {}\n",
    "transforms[\"camera_model\"] = \"OPENCV\"\n",
    "if USE_COMMON_INTRINSICS:\n",
    "    intrinsic_mean = intrinsics.mean(dim=0)\n",
    "    transforms[\"fl_x\"] = intrinsic_mean[0,0].item() * RESCALE_FACTOR\n",
    "    transforms[\"fl_y\"] = intrinsic_mean[1,1].item() * RESCALE_FACTOR\n",
    "    transforms[\"w\"] = width \n",
    "    transforms[\"h\"] = height \n",
    "    transforms[\"cx\"] = intrinsic_mean[0,2].item() * RESCALE_FACTOR\n",
    "    transforms[\"cy\"] = intrinsic_mean[1,2].item() * RESCALE_FACTOR\n",
    "\n",
    "transforms[\"frames\"] = []\n",
    "\n",
    "OPENGL = np.array([[1, 0, 0, 0],\n",
    "                    [0, -1, 0, 0],\n",
    "                    [0, 0, -1, 0],\n",
    "                    [0, 0, 0, 1]])\n",
    "\n",
    "for i in range(len(poses)):\n",
    "    if not((confidence_masks[i]==0).all()):\n",
    "        frame = {}\n",
    "        frame[\"file_path\"] = \"/\".join(images_array[i].split(\"/\")[-2:])\n",
    "        frame[\"transform_matrix\"] = poses[i].detach().cpu().numpy()\n",
    "        frame[\"transform_matrix\"] = np.linalg.inv(frame[\"transform_matrix\"])\n",
    "        #frame[\"transform_matrix\"] = OPENGL @ frame[\"transform_matrix\"]\n",
    "        #frame[\"transform_matrix\"] = np.linalg.inv(frame[\"transform_matrix\"])\n",
    "        frame[\"transform_matrix\"] = frame[\"transform_matrix\"].tolist()\n",
    "        frame[\"mask_path\"] = \"/\".join(masks_array[i].split(\"/\")[-2:])\n",
    "        transforms[\"frames\"].append(frame)\n",
    "        \n",
    "        if not USE_COMMON_INTRINSICS:\n",
    "            frame[\"fl_x\"] = intrinsics[i,0,0].item() * RESCALE_FACTOR\n",
    "            frame[\"fl_y\"] = intrinsics[i,1,1].item() * RESCALE_FACTOR\n",
    "            frame[\"cx\"] = intrinsics[i,0,2].item() * RESCALE_FACTOR\n",
    "            frame[\"cy\"] = intrinsics[i,1,2].item() * RESCALE_FACTOR\n",
    "            img = Image.open(images_array[i])\n",
    "            width, height = img.size\n",
    "            transforms[\"w\"] = width \n",
    "            transforms[\"h\"] = height \n",
    "\n",
    "#Save transform file\n",
    "with open(\"{}/transforms.json\".format(DATA_PATH), 'w') as f:\n",
    "    json.dump(transforms, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.0194e-02,  1.3301e-02, -7.9469e-04,  2.2044e-02,  1.6840e-01,\n",
      "        -1.2183e+00, -2.1366e-01,  3.1851e-02, -9.0499e-02, -7.3949e-02,\n",
      "         4.7403e-03, -1.3288e-03,  7.4363e-03, -2.7734e-02, -1.8639e-02,\n",
      "        -2.9648e-02, -1.5664e-02, -2.5353e-03, -1.6939e-02, -2.8425e-02,\n",
      "        -1.6713e-02, -8.4597e-03, -3.1244e-03,  6.5929e-03,  4.6599e-02,\n",
      "         1.5926e-02,  1.2407e-02, -7.0894e-04,  1.3250e-03,  4.6859e-03,\n",
      "        -1.1399e-03, -9.5838e-04, -2.0674e-03,  3.2499e-03, -5.9040e-03,\n",
      "        -8.2583e-03, -4.0776e-03,  8.7962e-03, -2.3547e-02, -8.8475e-04,\n",
      "        -2.1960e-02, -1.4557e-02,  7.9809e-03,  1.3969e-02,  2.0686e-02,\n",
      "         1.1537e-02,  2.2201e-02,  3.3209e-02,  4.0130e-02, -9.3726e-03],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import roma\n",
    "\n",
    "all_poses = torch.stack(list(scene.im_poses))\n",
    "Q = all_poses[:,:4]\n",
    "Q = torch.nn.functional.normalize(Q, p=2, dim=1)\n",
    "T = signed_expm1(all_poses[:,4:7])\n",
    "tf = roma.RigidUnitQuat(Q, T).normalize().to_homogeneous()\n",
    "\n",
    "OPENGL = torch.tensor([[1, 0, 0, 0],\n",
    "                       [0, -1, 0, 0],\n",
    "                       [0, 0, -1, 0],\n",
    "                       [0, 0, 0, 1]], dtype=torch.float32).to(device)\n",
    "\n",
    "tf = torch.matmul(tf, OPENGL)\n",
    "\n",
    "tf = roma.RigidUnitQuat(Q, T).normalize()\n",
    "print(tf.linear[:,0]/tf.linear[:,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
