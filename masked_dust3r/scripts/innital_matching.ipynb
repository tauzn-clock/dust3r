{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dust3r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/dust3r\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import open3d as o3d\n",
    "\n",
    "from dust3r.inference import inference_with_mask\n",
    "from dust3r.model import AsymmetricCroCo3DStereo\n",
    "from dust3r.utils.image import load_images\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n",
    "from dust3r.cloud_opt.base_opt import global_alignment_loop\n",
    "from masked_dust3r.scripts.utils.math import *\n",
    "from masked_dust3r.scripts.utils.image import *\n",
    "\n",
    "\n",
    "DATA_PATH = \"/dust3r/masked_dust3r/data/jackal_dense\"\n",
    "IMG_FILE_EXTENSION = \".png\"\n",
    "MASK_FILE_EXTENSION = \".png\"\n",
    "GAUSSIAN_SIGMA = 51.0 #TODO: Dynamically size GAUSSIAN_SIGMA based on mask size?\n",
    "INIT_FRAMES = 50\n",
    "NEW_FRAMES = 10\n",
    "PREVIOUS_FRAMES = 40\n",
    "TOTAL_FRAMES = 300\n",
    "\n",
    "INIT_WEIGHT_FOCAL = 0.1\n",
    "INIT_WEIGHT_Z = 0.0001 \n",
    "INIT_WEIGHT_ROT = 0.0001 * 0\n",
    "INIT_WEIGHT_TRANS_SMOOTHNESS = 0.0001 * 0\n",
    "INIT_WEIGHT_ROT_SMOOTHNESS = 0.001 * 0\n",
    "\n",
    "NEW_WEIGHT_FOCAL = 0.1\n",
    "NEW_WEIGHT_Z = 0.1\n",
    "NEW_WEIGHT_ROT = 0.1\n",
    "NEW_WEIGHT_TRANS_SMOOTHNESS = 0.00001\n",
    "NEW_WEIGHT_ROT_SMOOTHNESS = 0.00001\n",
    "\n",
    "USE_COMMON_INTRINSICS = False\n",
    "IS_FOCAL_FIXED = True\n",
    "FOCAL_LENGTH = 4.74\n",
    "\n",
    "device = 'cuda'\n",
    "batch_size = 1\n",
    "schedule = 'cosine'\n",
    "lr = 0.01\n",
    "niter = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading model from checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiating : AsymmetricCroCo3DStereo(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100', patch_embed_cls='PatchEmbedDust3R', img_size=(512, 512), head_type='dpt', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), landscape_only=False)\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "\n",
    "model_name = \"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "# you can put the path to a local checkpoint in model_name if needed\n",
    "model = AsymmetricCroCo3DStereo.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading a list of 20 images\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/0.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/1.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/2.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/3.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/4.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/5.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/6.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/7.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/8.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/9.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/10.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/11.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/12.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/13.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/14.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/15.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/16.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/17.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/18.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_dense/masked_images/19.png with resolution 1280x720 --> 512x288\n",
      " (Found 20 images)\n"
     ]
    }
   ],
   "source": [
    "images_array = []\n",
    "masks_array = []\n",
    "\n",
    "for i in range(0,20):\n",
    "    images_array.append(os.path.join(DATA_PATH,\"masked_images/{}{}\".format(i,IMG_FILE_EXTENSION)))\n",
    "    masks_array.append(os.path.join(DATA_PATH,\"masks/{}{}\".format(i,MASK_FILE_EXTENSION)))\n",
    "images = load_images(images_array, size=512, verbose=True)\n",
    "_,_,H,W = images[0][\"img\"].shape\n",
    "masks = load_masks(masks_array, H, W, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Inference with model on 200 image pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 200/200 [01:03<00:00,  3.13it/s]\n"
     ]
    }
   ],
   "source": [
    "pairs = make_pairs(images, scene_graph='swin-5', prefilter=None, symmetrize=True)\n",
    "output = inference_with_mask(pairs, model, device, masks, GAUSSIAN_SIGMA, batch_size=batch_size)\n",
    "del pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " init edge (7*,12*) score=1.204464316368103\n",
      " init edge (8*,12) score=1.1818453073501587\n",
      " init edge (7,11*) score=1.176344394683838\n",
      " init edge (7,5*) score=1.1713817119598389\n",
      " init edge (5,10*) score=1.170652151107788\n",
      " init edge (16*,12) score=1.1688141822814941\n",
      " init edge (12,17*) score=1.1647253036499023\n",
      " init edge (7,4*) score=1.1444718837738037\n",
      " init edge (2*,17) score=1.1384222507476807\n",
      " init edge (16,1*) score=1.1216689348220825\n",
      " init edge (8,13*) score=1.1856962442398071\n",
      " init edge (6*,11) score=1.1774686574935913\n",
      " init edge (14*,10) score=1.1745556592941284\n",
      " init edge (10,15*) score=1.1719152927398682\n",
      " init edge (9*,14) score=1.163331151008606\n",
      " init edge (4,19*) score=1.1493381261825562\n",
      " init edge (3*,19) score=1.1466799974441528\n",
      " init edge (15,0*) score=1.1243374347686768\n",
      " init edge (3,18*) score=1.1607537269592285\n",
      " init loss = 0.5039066076278687\n",
      "Global alignement - optimizing for:\n",
      "['pw_poses', 'im_depthmaps.0', 'im_depthmaps.1', 'im_depthmaps.2', 'im_depthmaps.3', 'im_depthmaps.4', 'im_depthmaps.5', 'im_depthmaps.6', 'im_depthmaps.7', 'im_depthmaps.8', 'im_depthmaps.9', 'im_depthmaps.10', 'im_depthmaps.11', 'im_depthmaps.12', 'im_depthmaps.13', 'im_depthmaps.14', 'im_depthmaps.15', 'im_depthmaps.16', 'im_depthmaps.17', 'im_depthmaps.18', 'im_depthmaps.19', 'im_poses.0', 'im_poses.1', 'im_poses.2', 'im_poses.3', 'im_poses.4', 'im_poses.5', 'im_poses.6', 'im_poses.7', 'im_poses.8', 'im_poses.9', 'im_poses.10', 'im_poses.11', 'im_poses.12', 'im_poses.13', 'im_poses.14', 'im_poses.15', 'im_poses.16', 'im_poses.17', 'im_poses.18', 'im_poses.19', 'im_focals.0', 'im_focals.1', 'im_focals.2', 'im_focals.3', 'im_focals.4', 'im_focals.5', 'im_focals.6', 'im_focals.7', 'im_focals.8', 'im_focals.9', 'im_focals.10', 'im_focals.11', 'im_focals.12', 'im_focals.13', 'im_focals.14', 'im_focals.15', 'im_focals.16', 'im_focals.17', 'im_focals.18', 'im_focals.19']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:05<00:00,  2.38it/s, lr=1.27413e-06 loss=0.318966]\n"
     ]
    }
   ],
   "source": [
    "#init_scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PlanePointCloudOptimizer)\n",
    "#loss = init_scene.compute_global_alignment(init=\"mst\", niter=niter, schedule='cosine', lr=lr)\n",
    "\n",
    "#scene = init_scene\n",
    "\n",
    "scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PlanePointCloudOptimizer, \n",
    "                        weight_focal = INIT_WEIGHT_FOCAL,\n",
    "                        weight_z = INIT_WEIGHT_Z ,\n",
    "                        weight_rot = INIT_WEIGHT_ROT  ,\n",
    "                        weight_trans_smoothness = INIT_WEIGHT_TRANS_SMOOTHNESS,\n",
    "                        weight_rot_smoothness = INIT_WEIGHT_ROT_SMOOTHNESS)\n",
    "#scene.im_poses = calculate_new_params(init_scene.im_poses,device)\n",
    "#scene.im_focals = init_scene.im_focals\n",
    "loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)\n",
    "\n",
    "#averge_focal = scene.get_focals().sum().item()/len(images_array)\n",
    "#fixed_focal = [averge_focal for _ in range(len(images_array))]\n",
    "#mask = [True for _ in range(len(images_array))]\n",
    "#scene.preset_focal(fixed_focal, mask)\n",
    "#loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)\n",
    "\n",
    "#scene.weight_focal = INIT_WEIGHT_FOCAL\n",
    "#scene.weight_z = INIT_WEIGHT_Z \n",
    "#scene.weight_rot = INIT_WEIGHT_ROT\n",
    "#scene.weight_trans_smoothness = INIT_WEIGHT_TRANS_SMOOTHNESS \n",
    "#scene.weight_rot_smoothness = INIT_WEIGHT_ROT_SMOOTHNESS\n",
    "#loss = global_alignment_loop(scene, niter=niter, schedule=schedule, lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = scene.imgs\n",
    "focals = scene.get_focals()\n",
    "poses = scene.get_im_poses()\n",
    "pts3d = scene.get_pts3d()\n",
    "confidence_masks = scene.get_masks()\n",
    "intrinsics = scene.get_intrinsics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if pointclouds folder exists\n",
    "#If exists, delete all files in the folder\n",
    "if os.path.exists(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH)):\n",
    "    for file in os.listdir(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH)):\n",
    "        os.remove(\"{DATA_PATH}/pointclouds/{file}\".format(DATA_PATH=DATA_PATH, file=file))\n",
    "        \n",
    "if not os.path.exists(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH)):\n",
    "    os.makedirs(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH))\n",
    "\n",
    "for i in range(len(images)):\n",
    "    pointcloud = pts3d[i].detach().cpu().numpy()\n",
    "    pointcloud = pointcloud.reshape(-1, 3)\n",
    "    color = imgs[i].reshape(-1, 3)\n",
    "    confidence_mask = confidence_masks[i].detach().cpu().numpy()\n",
    "    confidence_mask = confidence_mask.reshape(-1)\n",
    "    \n",
    "    masked_pointcloud = []\n",
    "    masked_color = []\n",
    "\n",
    "    for j in range(len(confidence_mask)):\n",
    "        if confidence_mask[j]:\n",
    "            masked_pointcloud.append(pointcloud[j])\n",
    "            masked_color.append(color[j])\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(masked_pointcloud)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(masked_color)\n",
    "    o3d.io.write_point_cloud(\"{DATA_PATH}/pointclouds/pointcloud{i}.ply\".format(DATA_PATH=DATA_PATH, i=i), pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create transform file\n",
    "#TODO: Per frame camera model?\n",
    "transforms = {}\n",
    "transforms[\"camera_model\"] = \"OPENCV\"\n",
    "if USE_COMMON_INTRINSICS:\n",
    "    averge_focal = focals.sum()/len(focals)\n",
    "    transforms[\"fl_x\"] = averge_focal.item()\n",
    "    transforms[\"fl_y\"] = averge_focal.item()\n",
    "\n",
    "    #Find size of images\n",
    "    img = Image.open(images_array[0])\n",
    "    width, height = img.size\n",
    "    transforms[\"w\"] = width\n",
    "    transforms[\"h\"] = height\n",
    "    transforms[\"cx\"] = width//2\n",
    "    transforms[\"cy\"] = height//2\n",
    "\n",
    "transforms[\"frames\"] = []\n",
    "\n",
    "OPENGL = np.array([[1, 0, 0, 0],\n",
    "                    [0, -1, 0, 0],\n",
    "                    [0, 0, -1, 0],\n",
    "                    [0, 0, 0, 1]])\n",
    "\n",
    "for i in range(len(poses)):\n",
    "    if not((confidence_masks[i]==0).all()):\n",
    "        frame = {}\n",
    "        frame[\"file_path\"] = \"/\".join(images_array[i].split(\"/\")[-2:])\n",
    "        #frame[\"transform_matrix\"] = np.linalg.inv(poses[i].detach().cpu().numpy()).tolist()\n",
    "        frame[\"transform_matrix\"] = np.dot(poses[i].detach().cpu().numpy(),OPENGL).tolist()\n",
    "        frame[\"mask_path\"] = \"/\".join(masks_array[i].split(\"/\")[-2:])\n",
    "        transforms[\"frames\"].append(frame)\n",
    "        \n",
    "        if not USE_COMMON_INTRINSICS:\n",
    "            frame[\"fl_x\"] = intrinsics[i,0,0].item()\n",
    "            frame[\"fl_y\"] = intrinsics[i,1,1].item()\n",
    "            frame[\"cx\"] = intrinsics[i,0,2].item()\n",
    "            frame[\"cy\"] = intrinsics[i,1,2].item()\n",
    "            img = Image.open(images_array[i])\n",
    "            width, height = img.size\n",
    "            transforms[\"w\"] = width\n",
    "            transforms[\"h\"] = height\n",
    "\n",
    "#Save transform file\n",
    "with open(\"{}/transforms.json\".format(DATA_PATH), 'w') as f:\n",
    "    json.dump(transforms, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file_path': 'masked_images/0.png', 'transform_matrix': [[0.9452298283576965, 0.25495293736457825, -0.20381249487400055, -0.058613043278455734], [0.25287798047065735, -0.9668051600456238, -0.03661201149225235, -0.009042802266776562], [-0.20638130605220795, -0.016932928934693336, -0.9783251285552979, 0.03645485267043114], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/0.png'}, {'file_path': 'masked_images/1.png', 'transform_matrix': [[0.9639269113540649, 0.19710680842399597, -0.17886775732040405, -0.05059170722961426], [0.19258451461791992, -0.9803603887557983, -0.042480118572711945, -0.010273641906678677], [-0.18372800946235657, 0.006500573828816414, -0.9829555749893188, 0.03507082909345627], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/1.png'}, {'file_path': 'masked_images/2.png', 'transform_matrix': [[0.9765200614929199, 0.17060469090938568, -0.13153930008411407, -0.03962356597185135], [0.17624182999134064, -0.9838138818740845, 0.03238897770643234, 0.008964123204350471], [-0.12388448417186737, -0.05481121689081192, -0.9907816648483276, 0.016380473971366882], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/2.png'}, {'file_path': 'masked_images/3.png', 'transform_matrix': [[0.982420802116394, 0.1368432641029358, -0.12697729468345642, -0.03508290275931358], [0.14089453220367432, -0.9897468090057373, 0.023449363186955452, 0.006446129642426968], [-0.12246648967266083, -0.04092755168676376, -0.9916283488273621, 0.031366873532533646], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/3.png'}, {'file_path': 'masked_images/4.png', 'transform_matrix': [[0.990118682384491, 0.10380225628614426, -0.09428707510232925, -0.025133715942502022], [0.10817045718431473, -0.9932255744934082, 0.04245048016309738, 0.011507336981594563], [-0.0892418846487999, -0.05223008990287781, -0.99463951587677, 0.032027073204517365], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/4.png'}, {'file_path': 'masked_images/5.png', 'transform_matrix': [[0.9960942268371582, 0.07001101970672607, -0.05380241200327873, -0.014999262988567352], [0.07084913551807404, -0.9973911643028259, 0.013829095289111137, 0.005267876200377941], [-0.05269386246800423, -0.01758693717420101, -0.998455822467804, 0.00047881121281534433], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/5.png'}, {'file_path': 'masked_images/6.png', 'transform_matrix': [[0.9990925192832947, 0.03527911379933357, -0.023862501606345177, -0.004825121257454157], [0.03486524149775505, -0.9992379546165466, -0.01754322648048401, -0.003448581788688898], [-0.02446322701871395, 0.016695335507392883, -0.9995612502098083, 0.0031991847790777683], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/6.png'}, {'file_path': 'masked_images/7.png', 'transform_matrix': [[0.9999244809150696, -0.0035306105855852365, -0.011779223568737507, 0.0], [-0.0035816647578030825, -0.9999843239784241, -0.004315976519137621, 0.0], [-0.011763800866901875, 0.004357839468866587, -0.9999213814735413, 0.0], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/7.png'}, {'file_path': 'masked_images/8.png', 'transform_matrix': [[0.9990538358688354, -0.030324140563607216, 0.031176084652543068, 0.013685282319784164], [-0.03010919876396656, -0.9995196461677551, -0.00734106358140707, -0.0010093257296830416], [0.03138371929526329, 0.00639543030411005, -0.9994869828224182, 0.0030938019044697285], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/8.png'}, {'file_path': 'masked_images/9.png', 'transform_matrix': [[0.9946913123130798, -0.06971858441829681, 0.07568691670894623, 0.027896737679839134], [-0.06781449913978577, -0.9973203539848328, -0.02744564786553383, -0.006069064140319824], [0.07739757001399994, 0.022167276591062546, -0.9967538118362427, 0.00400595972314477], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/9.png'}, {'file_path': 'masked_images/10.png', 'transform_matrix': [[0.9860715866088867, -0.11278333514928818, 0.1222405806183815, 0.04327293485403061], [-0.11178701370954514, -0.9936189651489258, -0.01500045508146286, -0.0031745336018502712], [0.12315236777067184, 0.0011266120709478855, -0.9923871159553528, 0.0010041410569101572], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/10.png'}, {'file_path': 'masked_images/11.png', 'transform_matrix': [[0.9806917309761047, -0.15982212126255035, 0.1126975268125534, 0.04188912734389305], [-0.15501229465007782, -0.9866322875022888, -0.050279561430215836, -0.013852037489414215], [0.11922679841518402, 0.03183924779295921, -0.9923564791679382, 0.0002523604780435562], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/11.png'}, {'file_path': 'masked_images/12.png', 'transform_matrix': [[0.9657688736915588, -0.18706755340099335, 0.17971090972423553, 0.06026022881269455], [-0.18936456739902496, -0.981896698474884, -0.00444385688751936, -0.0013633026974275708], [0.17728887498378754, -0.0297391414642334, -0.9837093353271484, 0.016517216339707375], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/12.png'}, {'file_path': 'masked_images/13.png', 'transform_matrix': [[0.9493640065193176, -0.23436449468135834, 0.20923908054828644, 0.07163307070732117], [-0.23020698130130768, -0.9721298217773438, -0.04436302185058594, -0.012540791183710098], [0.2138047069311142, -0.006051644682884216, -0.9768575429916382, 0.011364164762198925], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/13.png'}, {'file_path': 'masked_images/14.png', 'transform_matrix': [[0.9276599884033203, -0.2693234980106354, 0.25867322087287903, 0.09004370868206024], [-0.25677433609962463, -0.9630032777786255, -0.08180250227451324, -0.02371937967836857], [0.27113446593284607, 0.009464263916015625, -0.962494969367981, 0.005218200385570526], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/14.png'}, {'file_path': 'masked_images/15.png', 'transform_matrix': [[0.9086669087409973, -0.29274436831474304, 0.297699898481369, 0.09344369173049927], [-0.300841361284256, -0.9534778594970703, -0.01935066655278206, -0.006259728688746691], [0.28951504826545715, -0.07197712361812592, -0.9544633626937866, 0.03756837919354439], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/15.png'}, {'file_path': 'masked_images/16.png', 'transform_matrix': [[0.8909865021705627, -0.33127254247665405, 0.310486376285553, 0.10509396344423294], [-0.3265708088874817, -0.9426769018173218, -0.0686432346701622, -0.020521245896816254], [0.3154279589653015, -0.04023558646440506, -0.9480962753295898, 0.018277853727340698], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/16.png'}, {'file_path': 'masked_images/17.png', 'transform_matrix': [[0.8651339411735535, -0.367716908454895, 0.3410686254501343, 0.10494600981473923], [-0.36564236879348755, -0.9278942346572876, -0.07292608916759491, -0.020221929997205734], [0.343291699886322, -0.06161828339099884, -0.9372055530548096, 0.05075186491012573], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/17.png'}, {'file_path': 'masked_images/18.png', 'transform_matrix': [[0.8509511351585388, -0.42405155301094055, 0.30993905663490295, 0.096986323595047], [-0.4095276892185211, -0.9051441550254822, -0.11402158439159393, -0.03024153783917427], [0.3288905918598175, -0.029901836067438126, -0.9438943862915039, 0.05451248213648796], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/18.png'}, {'file_path': 'masked_images/19.png', 'transform_matrix': [[0.8233540058135986, -0.45845088362693787, 0.33453091979026794, 0.10582754015922546], [-0.4439731538295746, -0.8874855041503906, -0.12352055311203003, -0.03341519460082054], [0.353519469499588, -0.046821609139442444, -0.9342546463012695, 0.05451273173093796], [0.0, 0.0, 0.0, 1.0]], 'mask_path': 'masks/19.png'}]\n"
     ]
    }
   ],
   "source": [
    "print(transforms[\"frames\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0051, -0.0125,  0.0221,  0.0162,  0.0238,  0.0079, -0.0086, -0.0022,\n",
      "        -0.0034, -0.0124, -0.0041, -0.0207,  0.0064, -0.0098, -0.0237,  0.0138,\n",
      "        -0.0075, -0.0030, -0.0227, -0.0210], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import roma\n",
    "\n",
    "all_poses = torch.stack(list(scene.im_poses))\n",
    "Q = all_poses[:,:4]\n",
    "Q = torch.nn.functional.normalize(Q, p=2, dim=1)\n",
    "T = signed_expm1(all_poses[:,4:7])\n",
    "tf = roma.RigidUnitQuat(Q, T).normalize().to_homogeneous()\n",
    "\n",
    "OPENGL = torch.tensor([[1, 0, 0, 0],\n",
    "                       [0, -1, 0, 0],\n",
    "                       [0, 0, -1, 0],\n",
    "                       [0, 0, 0, 1]], dtype=torch.float32).to(device)\n",
    "\n",
    "tf = torch.matmul(tf, OPENGL)\n",
    "\n",
    "tf = roma.RigidUnitQuat(Q, T).normalize()\n",
    "print(tf.linear[:,0]/tf.linear[:,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
