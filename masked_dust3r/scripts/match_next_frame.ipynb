{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dust3r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#Set current location to the location of the script\n",
    "os.chdir(\"/dust3r\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Warning, cannot find cuda-compiled version of RoPE2D, using a slow pytorch version instead\n"
     ]
    }
   ],
   "source": [
    "#Display imgs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from dust3r.inference import inference, inference_with_mask\n",
    "from dust3r.model import AsymmetricCroCo3DStereo\n",
    "from dust3r.utils.image import load_images\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n",
    "\n",
    "DATA_PATH = \"/dust3r/masked_dust3r/data/jackal_training_data_0\"\n",
    "IMG_FILE_EXTENSION = \".png\"\n",
    "MASK_FILE_EXTENSION = \".png\"\n",
    "PREV_FRAME = 4\n",
    "device = 'cuda'\n",
    "batch_size = 1\n",
    "schedule = 'cosine'\n",
    "lr = 0.01\n",
    "niter = 300\n",
    "\n",
    "#Open transforms\n",
    "with open(\"{}/transforms.json\".format(DATA_PATH)) as f:\n",
    "    transforms = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading model from checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiating : AsymmetricCroCo3DStereo(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100', patch_embed_cls='PatchEmbedDust3R', img_size=(512, 512), head_type='dpt', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), landscape_only=False)\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model_name = \"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "# you can put the path to a local checkpoint in model_name if needed\n",
    "model = AsymmetricCroCo3DStereo.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading a list of 5 images\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/6.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/7.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/8.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/9.png with resolution 1280x720 --> 512x288\n",
      " - adding /dust3r/masked_dust3r/data/jackal_training_data_0/masked_images/10.png with resolution 1280x720 --> 512x288\n",
      " (Found 5 images)\n"
     ]
    }
   ],
   "source": [
    "# load_images can take a list of images or a directory\n",
    "\n",
    "images_array = []\n",
    "masks_array = []\n",
    "\n",
    "preset_focal = [transforms[\"fl_x\"] for _ in range(PREV_FRAME+1)]\n",
    "preset_pose = []\n",
    "preset_mask = [True for _ in range(PREV_FRAME+1)]\n",
    "preset_mask[-1] = False\n",
    "\n",
    "for i in range(-PREV_FRAME,0):\n",
    "    images_array.append(os.path.join(DATA_PATH,transforms[\"frames\"][i][\"file_path\"]))\n",
    "    masks_array.append(os.path.join(DATA_PATH,transforms[\"frames\"][i][\"mask_path\"]))\n",
    "    preset_pose.append(np.array(transforms[\"frames\"][i][\"transform_matrix\"]))\n",
    "\n",
    "images_array.append(os.path.join(DATA_PATH,\"masked_images/{}{}\".format(len(transforms[\"frames\"]),IMG_FILE_EXTENSION)))\n",
    "masks_array.append(os.path.join(DATA_PATH,\"masks/{}{}\".format(len(transforms[\"frames\"]),MASK_FILE_EXTENSION)))\n",
    "preset_pose.append(np.eye(4))\n",
    "\n",
    "images = load_images(images_array, size=512, verbose=True)\n",
    "\n",
    "masks = []\n",
    "\n",
    "for i in range(len(masks_array)):\n",
    "    #Open as mask and load to gpu\n",
    "    mask = Image.open(masks_array[i]).convert('L')\n",
    "    #Resize to match image size\n",
    "    _,_,H,W = images[i][\"img\"].shape\n",
    "    mask = mask.resize((W,H))\n",
    "\n",
    "    mask = np.array(mask)\n",
    "    mask = torch.tensor(mask).to(device)/255\n",
    "    masks.append(mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Inference with model on 20 image pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "pairs = make_pairs(images, scene_graph='complete', prefilter=None, symmetrize=True)\n",
    "output = inference_with_mask(pairs, model, device, masks, batch_size=batch_size)\n",
    "#output = inference(pairs, model, device, batch_size=batch_size)\n",
    "\n",
    "view1, pred1 = output['view1'], output['pred1']\n",
    "view2, pred2 = output['view2'], output['pred2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (setting focal #0 = 491.638671875)\n",
      " (setting focal #1 = 491.638671875)\n",
      " (setting focal #2 = 491.638671875)\n",
      " (setting focal #3 = 491.638671875)\n",
      " (setting pose #0 = [ 0.08779736 -0.01404059  0.03852846])\n",
      " (setting pose #1 = [ 0.11026087 -0.03048317  0.03406788])\n",
      " (setting pose #2 = [ 0.11806521 -0.04100279  0.08014522])\n",
      " (setting pose #3 = [ 0.11607814 -0.04993593  0.11948954])\n"
     ]
    }
   ],
   "source": [
    "scene = global_aligner(output, device=device, mode=GlobalAlignerMode.ModularPointCloudOptimizer)\n",
    "scene.preset_focal(preset_focal, preset_mask)\n",
    "scene.preset_pose(preset_pose, preset_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " init edge (0*,3*) score=1.149208664894104\n",
      " init edge (0,4*) score=1.1482036113739014\n",
      " init edge (0,2*) score=1.1307114362716675\n",
      " init edge (0,1*) score=1.1098133325576782\n",
      " init loss = 0.006146085914224386\n",
      "Global alignement - optimizing for:\n",
      "['pw_poses', 'im_depthmaps.0', 'im_depthmaps.1', 'im_depthmaps.2', 'im_depthmaps.3', 'im_depthmaps.4', 'im_poses.4', 'im_focals.4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:27<00:00, 10.75it/s, lr=1.27413e-06 loss=0.000130587]\n"
     ]
    }
   ],
   "source": [
    "loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)\n",
    "\n",
    "imgs = scene.imgs\n",
    "focals = scene.get_focals()\n",
    "poses = scene.get_im_poses()\n",
    "pts3d = scene.get_pts3d()\n",
    "confidence_masks = scene.get_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if pointclouds folder exists\n",
    "#If exists, delete all files in the folder\n",
    "if os.path.exists(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH)):\n",
    "    for file in os.listdir(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH)):\n",
    "        os.remove(\"{DATA_PATH}/pointclouds/{file}\".format(DATA_PATH=DATA_PATH, file=file))\n",
    "        \n",
    "if not os.path.exists(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH)):\n",
    "    os.makedirs(\"{DATA_PATH}/pointclouds\".format(DATA_PATH=DATA_PATH))\n",
    "\n",
    "for i in range(len(images)):\n",
    "    pointcloud = pts3d[i].detach().cpu().numpy()\n",
    "    pointcloud = pointcloud.reshape(-1, 3)\n",
    "    color = imgs[i].reshape(-1, 3)\n",
    "    confidence_mask = confidence_masks[i].detach().cpu().numpy()\n",
    "    confidence_mask = confidence_mask.reshape(-1)\n",
    "    \n",
    "    masked_pointcloud = []\n",
    "    masked_color = []\n",
    "\n",
    "    for j in range(len(confidence_mask)):\n",
    "        if confidence_mask[j]:\n",
    "            masked_pointcloud.append(pointcloud[j])\n",
    "            masked_color.append(color[j])\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(masked_pointcloud)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(masked_color)\n",
    "    o3d.io.write_point_cloud(\"{DATA_PATH}/pointclouds/pointcloud{i}.ply\".format(DATA_PATH=DATA_PATH, i=i), pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 511.5, 287.5, -0.5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf1 = confidence_masks[1].cpu().numpy()\n",
    "conf2 = confidence_masks[2].cpu().numpy()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(conf1, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(conf2, cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_frame = {\n",
    "    \"file_path\" : \"/\".join(images_array[-1].split(\"/\")[-2:]),\n",
    "    \"transform_matrix\" : poses[-1].detach().cpu().numpy().tolist(),\n",
    "    \"mask_path\" : \"/\".join(masks_array[-1].split(\"/\")[-2:])\n",
    "}\n",
    "transforms[\"frames\"].append(new_frame)\n",
    "\n",
    "with open(\"{DATA_PATH}/transforms.json\".format(DATA_PATH=DATA_PATH), \"w\") as f:\n",
    "    json.dump(transforms, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
